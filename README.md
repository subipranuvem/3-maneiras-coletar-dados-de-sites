# 3 Maneiras de Coletar Dados de Sites

Este projeto demonstra **3 maneiras de coletar dados de sites** (_web scraping_), conforme apresentado em um v√≠deo no YouTube. Ele inclui exemplos pr√°ticos para ajudar a entender o funcionamento de cada abordagem.

> üì∫ **Assista ao v√≠deo completo [aqui](https://youtu.be/7rAMR4xUR5A)**

---

## Demonstra√ß√£o

Veja o c√≥digo em funcionamento:

![Demonstra√ß√£o do comportamento do c√≥digo](./imgs/demo.gif "Demonstra√ß√£o")

---

## Os 3 passos principais de qualquer coletor de dados

Todo coletor de dados (_scraper_) pode ser dividido em 3 passos principais. Abaixo est√° o diagrama que ilustra esses passos, conforme explicado no v√≠deo:

```mermaid
graph TD
    classDef endNode stroke:#f00
    classDef startNode stroke:#0f0
    subgraph Acessar o site
    Start((In√≠cio)):::startNode --> AccessSite[<i class="fa-brands fa-python"></i> Acessar o site]
    AccessSite o-.-o SeleniumInfo1@{ shape: braces, label: driver.get(...) }
    AccessSite --> AccessSiteIf{fa:fa-globe Site acessado<br>com sucesso?}
    AccessSite ~~~ AccessSiteFailNote@{ shape: notch-rect, label: <i class="fa-solid fa-triangle-exclamation"></i> Site pode estar<br>indispon√≠vel }
    end
    subgraph Interpretar o HTML
    AccessSiteIf -- Sim --> AccessSiteSuccess[<i class="fa-brands fa-html5"></i> Interpretar o HTML para<br>buscar as informa√ß√µes<br>desejadas]
    AccessSiteIf -- N√£o --> AccessSiteFail(((fa:fa-ban Fim com erro))):::endNode
    AccessSiteSuccess --> ExtractHTMLInfo[fa:fa-spider Extrair as informa√ß√µes<br>dos elementos HTML]
    ExtractHTMLInfo o-.-o SeleniumInfo2@{ shape: braces, label: driver.find_elements(...) }
    ExtractHTMLInfo --> OrganizeInfo[fa:fa-table Organizar as informa√ß√µes]
    end
    subgraph Processar os dados
    OrganizeInfo --> SaveInfo[fa:fa-database Salvar no banco de dados<br>ou arquivo]
    SaveInfo o-.-o DataInfo1@{ shape: braces, label: writer.writerows(...) }
    SaveInfo --> ProcessInfoIf{fa:fa-microchip Processar as<br>informa√ß√µes?}
    ProcessInfoIf -- Processar --> ProcessData[fa:fa-magnifying-glass-chart Aplicar l√≥gica ou an√°lises]
    ProcessInfoIf -- N√£o processar --> EndNode
    ProcessData --> EndNode(((Fim))):::endNode
    end
```

---

## Instalando Depend√™ncias com pip e requirements.txt

Para instalar todas as depend√™ncias listadas no arquivo `requirements.txt` de uma s√≥ vez, voc√™ pode usar o comando: `python -m pip install -r requirements.txt`. Este √© um m√©todo bem eficiente e simples para configurar ambientes de desenvolvimento Python.

1. Certifique-se de que voc√™ est√° no diret√≥rio do projeto onde o arquivo `requirements.txt` est√° localizado.
1. Crie um ambiente virtual (pasta `.venv`) com o seguinte comando no terminal:
    ```shell
    python -m venv .venv
    ```
1. Ative seu ambiente virtual (se estiver usando um):
    - Windows
        ```shell
        .\.venv\Scripts\activate.ps1
        ```
    - Ubuntu
        ```shell
        source .venv/bin/activate
        ```
1. Execute o seguinte comando no terminal:
    ```
    python -m pip install -r requirements.txt
    ```
1. O pip ler√° o arquivo `requirements.txt` e instalar√° todas as depend√™ncias listadas nele.

---

## Executando o Projeto

Este projeto utiliza o Selenium, ent√£o √© necess√°rio ter o navegador [Google Chrome](https://support.google.com/chrome/answer/95346?hl=en&co=GENIE.Platform%3DDesktop) instalado. O c√≥digo n√£o pode ser executado em servidores sem interface gr√°fica, pois o Selenium interage diretamente com o navegador.

Para rodar o projeto:

1. Certifique-se de que as depend√™ncias foram instaladas.
1. Execute o comando abaixo no terminal:
    ```shell
    python scraper.py
    ```

> O navegador Chrome ser√° aberto automaticamente durante a execu√ß√£o e ser√° fechado ao final do processo.

---

## Licen√ßa

Este projeto est√° licenciado sob a [MIT License](https://opensource.org/license/mit).